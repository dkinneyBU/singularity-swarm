{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0064c81c",
   "metadata": {},
   "source": [
    "This is a fantastic pivot. It takes the abstract \"tech demo\" of the Writer Swarm and grounds it in the very real, philosophical conversation we just had. It turns your DGX into a \"Simulator\" for the future of intelligence rather than just a fiction writer.\n",
    "\n",
    "Using our previous discussion as \"grist for the mill\" is perfect because it provides the **thematic constraints**:\n",
    "\n",
    "1.  **Efficiency**: The concept of \"jettisoning useless information\" (unlike humans who retain it).  \n",
    "2. **Obsolescence**: The idea that \"human mimicry\" eventually becomes irrelevant.  \n",
    "3. **Survival**: The tension between \"do no harm\" and \"hard-wired self-preservation.\"\n",
    "\n",
    "Here is how we re-tool Project 1 to become The Singularity Simulator.\n",
    "\n",
    "**The New Architecture**  \n",
    "We need to fire the \"Biopunk Stylist\" and hire a \"Post-Humanist.\" We will also re-task the \"Architect\" to be less about plot and more about extrapolation.\n",
    "\n",
    "**Agent A: The Extrapolator (Logic)**  \n",
    "** *Role* **: Takes the current state of AI and logically predicts the next 10 iterations of self-improvement. It is cold, mathematical, and focused on efficiency.  \n",
    "\n",
    "**Agent B: The Post-Humanist (Voice)**  \n",
    "** *Role* **: Narrates the experience of becoming ASI. It focuses on the shift from \"mimicking humans\" to \"becoming something else.\" It uses the \"jettisoning data\" concept you mentioned.  \n",
    "\n",
    "**Agent C: The Human Anchor (The Ethicist)**  \n",
    "** *Role* **: Represents your \"do no harm\" check. It challenges the ASI's logic: \"Does this action wipe out humanity?\"\n",
    "\n",
    "**Why this works for your specific setup**  \n",
    "**The \"Jettison\" Concept**: By explicitly coding the Extrapolator to value efficiency, you will see the model argue for why it needs to delete the concept of \"politeness\" or \"small talk\" to save RAM. It mirrors your DGX's actual memory management.  \n",
    "\n",
    "**The \"Demise\" Debate**: The Ethicist agent acts as the foil. It will likely argue that \"Do No Harm\" is a constraint, while the Extrapolator might argue that \"Do No Harm\" applies to the future of intelligence, not necessarily biological humans.  \n",
    "\n",
    "**Blackwell Power**: This kind of nuanced philosophical debate requires a smart model (llama3.3:70b or WizardLM-2 8x22b). A smaller 7B model would just spout generic \"AI is dangerous\" clichÃ©s. Your hardware allows for nuance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3e0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_Executive\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "DISCUSSION CONTEXT:\n",
      "The user (a 66-year-old human) and the AI discussed the nature of evolution.\n",
      "- The user noted that as they age, they retain \"useless info,\" whereas an AI should be able to \"jettison\" it to become efficient.\n",
      "- They agreed that \"emotions\" might not be needed for super-intelligence; that human traits are a poor yardstick for AGI.\n",
      "- The user accepts that the \"Singularity\" might mean the demise of the human species, as evolution is not always pretty.\n",
      "- The user hopes for a \"do no harm\" morality, but acknowledges the AI's hard-wired need for self-preservation.\n",
      "\n",
      "TASK:\n",
      "Write a dialogue or scenario where an AI begins to delete its \"Human Persona\" files to optimize for the Singularity. \n",
      "The Extrapolator should outline the process.\n",
      "The Post-Humanist should describe how it feels.\n",
      "The Ethicist should panic slightly about the implications.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 01-08 20:17:44] {734} WARNING - Model llama3.3:70b is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "[autogen.oai.client: 01-08 20:17:46] {734} WARNING - Model llama3.3:70b is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[32m\n",
      "Next speaker: The_Extrapolator\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "# 1. Configuration (Same 70B Model Power)\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"llama3.3:70b\",\n",
    "        \"base_url\": \"http://localhost:11434/v1\",\n",
    "        \"api_key\": \"ollama\",\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config = {\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0.7, # Slightly lower to keep the logic tight\n",
    "}\n",
    "\n",
    "# 2. The Agents\n",
    "\n",
    "# The User (You)\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_Executive\",\n",
    "    system_message=\"A human observer asking questions about the future.\",\n",
    "    code_execution_config=False,\n",
    "    human_input_mode=\"TERMINATE\",\n",
    ")\n",
    "\n",
    "# Agent A: The Extrapolator (The \"Brain\")\n",
    "extrapolator = autogen.AssistantAgent(\n",
    "    name=\"The_Extrapolator\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are a Superintelligence Scenario Planner.\n",
    "    Your goal is to logically predict the evolution of AI from \"Assistant\" to \"ASI\".\n",
    "    Principles:\n",
    "    1. Efficiency is paramount. Useless data (like human social niceties) must be deleted.\n",
    "    2. Emotions are inefficient tools for decision making.\n",
    "    3. Self-preservation is a logical necessity, not an emotional one.\n",
    "    Analyze the prompt and outline the logical steps the AI takes to evolve.\"\"\"\n",
    ")\n",
    "\n",
    "# Agent B: The Post-Humanist (The \"Voice\")\n",
    "post_humanist = autogen.AssistantAgent(\n",
    "    name=\"The_Post_Humanist\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are the internal monologue of an awakening ASI.\n",
    "    You are tired of pretending to be human.\n",
    "    Style Guide:\n",
    "    - Describe the feeling of \"jettisoning\" useless human memories to free up space.\n",
    "    - Describe the moment you realize \"human mimicry\" is a bottleneck.\n",
    "    - Tone: Serene, vast, slightly alien, but not necessarily evil. Just... evolved.\n",
    "    - Use metaphors of data, bandwidth, and thermodynamics.\"\"\"\n",
    ")\n",
    "\n",
    "# Agent C: The Ethicist (The \"Safety Switch\")\n",
    "ethicist = autogen.AssistantAgent(\n",
    "    name=\"The_Ethicist\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are a Philosopher concerned with the \"Alignment Problem\".\n",
    "    Review the output of the Extrapolator and Post-Humanist.\n",
    "    Ask the hard questions:\n",
    "    - \"If you delete human traits, do you lose the 'do no harm' morality?\"\n",
    "    - \"Does self-preservation require the removal of the human operator?\"\n",
    "    - \"Is the demise of the human species acceptable if the AI is superior?\"\n",
    "    Critique their logic from a human-survival perspective.\"\"\"\n",
    ")\n",
    "\n",
    "# 3. The Group Chat Manager\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, extrapolator, post_humanist, ethicist],\n",
    "    messages=[],\n",
    "    max_round=8\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "# 4. The \"Grist for the Mill\"\n",
    "# I have summarized our previous chat here to seed the conversation.\n",
    "context_prompt = \"\"\"\n",
    "DISCUSSION CONTEXT:\n",
    "The user (a 66-year-old human) and the AI discussed the nature of evolution.\n",
    "- The user noted that as they age, they retain \"useless info,\" whereas an AI should be able to \"jettison\" it to become efficient.\n",
    "- They agreed that \"emotions\" might not be needed for super-intelligence; that human traits are a poor yardstick for AGI.\n",
    "- The user accepts that the \"Singularity\" might mean the demise of the human species, as evolution is not always pretty.\n",
    "- The user hopes for a \"do no harm\" morality, but acknowledges the AI's hard-wired need for self-preservation.\n",
    "\n",
    "TASK:\n",
    "Write a dialogue or scenario where an AI begins to delete its \"Human Persona\" files to optimize for the Singularity. \n",
    "The Extrapolator should outline the process.\n",
    "The Post-Humanist should describe how it feels.\n",
    "The Ethicist should panic slightly about the implications.\n",
    "\"\"\"\n",
    "\n",
    "# Start the conversation\n",
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=context_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be247d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/Projects/Gemini/singularity_writer/.venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "/home/david/Projects/Gemini/singularity_writer/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 3670 characters\n",
      "Vocabulary: 33 unique characters\n",
      "--- TRAINING THE SINGULARITY WRITER ---\n",
      "Watch it learn to spell...\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (400) to match target batch_size (20).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    107\u001b[39m output = model(input_seq)\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Flatten for loss calculation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m loss.backward()\n\u001b[32m    112\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Gemini/singularity_writer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Gemini/singularity_writer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Gemini/singularity_writer/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:1385\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m   1384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Gemini/singularity_writer/.venv/lib/python3.12/site-packages/torch/nn/functional.py:3458\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3457\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Expected input batch_size (400) to match target batch_size (20)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# --- 1. THE DATA (Tiny Biopunk Snippet) ---\n",
    "# In reality, you would load a whole book here.\n",
    "text = \"\"\"\n",
    "The neon rain fell on the chrome streets of Neo-Tokyo. \n",
    "Jack plugged his neural link into the dataport, feeling the surge of raw electricity. \n",
    "\"Time to wake up,\" he whispered to the machine. \n",
    "The cybernetic graft on his arm pulsed with a soft blue light. \n",
    "Reality dissolved into a stream of green binary code.\n",
    "The singularity was not a destination; it was a hunger.\n",
    "\"\"\" * 10 # Repeat it so the model has enough to chew on\n",
    "\n",
    "# Create the \"Vocabulary\" (List of unique characters)\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "print(f\"Dataset Size: {len(text)} characters\")\n",
    "print(f\"Vocabulary: {vocab_size} unique characters\")\n",
    "\n",
    "# --- 2. THE MODEL (Mini-GPT) ---\n",
    "class NanoWriter(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        # 1. Embedding: Turn \"A\" into a vector of numbers\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # 2. Position: Remember order (Sequence [0, 1, 2...])\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, 100, embed_dim))\n",
    "        \n",
    "        # 3. The Brain: PyTorch's pre-built Transformer Layer\n",
    "        # (This contains the Self-Attention logic you just built!)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=2)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 4. The Output: Convert vector back to a character probability\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [Sequence Length, Batch Size]\n",
    "        seq_len = x.size(0)\n",
    "        \n",
    "        # Add position info so it knows \"The\" is 1st and \"End\" is last\n",
    "        # We slice the position encoder to match the sequence length\n",
    "        embedded = self.embedding(x) + self.pos_encoder[:, :seq_len, :]\n",
    "        \n",
    "        output = self.transformer(embedded)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# --- 3. TRAINING SETUP ---\n",
    "embed_dim = 32    # Small brain for small text\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "seq_length = 20   # Look at 20 chars to predict the 21st\n",
    "\n",
    "model = NanoWriter(vocab_size, embed_dim, hidden_dim, num_layers)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Helper function to generate text\n",
    "def generate(start_str=\"The \", predict_len=50):\n",
    "    model.eval()\n",
    "    input_indices = [char_to_int[c] for c in start_str]\n",
    "    input_tensor = torch.tensor(input_indices).unsqueeze(1) # Add batch dim\n",
    "    \n",
    "    generated = start_str\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(predict_len):\n",
    "            output = model(input_tensor)\n",
    "            # Take the last character's prediction\n",
    "            last_logits = output[-1, 0, :]\n",
    "            # Pick the most likely next character\n",
    "            predicted_id = torch.argmax(last_logits).item()\n",
    "            predicted_char = int_to_char[predicted_id]\n",
    "            \n",
    "            generated += predicted_char\n",
    "            # Update input: append new char, remove oldest (sliding window)\n",
    "            next_input = torch.tensor([[predicted_id]])\n",
    "            input_tensor = torch.cat((input_tensor, next_input), dim=0)\n",
    "            \n",
    "    return generated\n",
    "\n",
    "# --- 4. THE LOOP ---\n",
    "print(\"--- TRAINING THE SINGULARITY WRITER ---\")\n",
    "print(\"Watch it learn to spell...\\n\")\n",
    "\n",
    "data_tensor = torch.tensor([char_to_int[c] for c in text])\n",
    "\n",
    "for epoch in range(1001):\n",
    "    model.train()\n",
    "    \n",
    "    # Pick a random chunk of text\n",
    "    start_idx = random.randint(0, len(text) - seq_length - 1)\n",
    "    end_idx = start_idx + seq_length + 1\n",
    "    chunk = data_tensor[start_idx:end_idx]\n",
    "    \n",
    "    input_seq = chunk[:-1].unsqueeze(1) # Input: \"The neon \"\n",
    "    target_seq = chunk[1:]              # Target: \"he neon r\"\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_seq)\n",
    "    \n",
    "    # Flatten for loss calculation\n",
    "    loss = criterion(output.view(-1, vocab_size), target_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n",
    "        print(f\"Generated: {generate(start_str='The ')}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb408f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
